{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Results\n",
    "## Craniosynostosis Detection Group \n",
    "Pouria Mashour, Carson McLean, Chantal Shaib, Devin Singh, Seung-Eun Yi\n",
    "\n",
    "**Note** - the plot images which follow each code block are embedded via Markdown using relative paths and therefore this `.ipynb` (and corresponding HTML version) must be kept within the project repository. Otherwise, the output images will be missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
       "\n",
       "// Rerun this cell to update Table of Contents\n",
       "// https://github.com/kmahelona/ipython_notebook_goodies"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "\n",
    "// Rerun this cell to update Table of Contents\n",
    "// https://github.com/kmahelona/ipython_notebook_goodies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D GAN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GAN_2D import WGAN\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Training 2DGAN:')\n",
    "  \n",
    "    parser.add_argument('--epochs', type=int, help='number of epochs for training')\n",
    "    parser.add_argument('--batch_size', type=int, help='batch size for training')\n",
    "    parser.add_argument('--save_int', type=int, help='interval to save data at')\n",
    "    parser.add_argument('--out_file', type=str, help='file to save model (e.g. model.h5)')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def pad_image(data):\n",
    "        max_size = max(data[0].shape[0], data[0].shape[1])\n",
    "        smaller_size = min(data[0].shape[0], data[0].shape[1])\n",
    "\n",
    "        max_size = 224\n",
    "\n",
    "        pad_size_l = (max_size - smaller_size)//2\n",
    "        pad_size_r = smaller_size + (max_size - smaller_size)//2 \n",
    "\n",
    "        print(pad_size_l)\n",
    "        print(pad_size_r)\n",
    "        uniform_data = np.zeros((data.shape[0], max_size, max_size, data.shape[-1]))\n",
    "        uniform_data[:,pad_size_l:pad_size_r,pad_size_l:pad_size_r,:] = data\n",
    "\n",
    "def load_data(num):\n",
    "    \"\"\"\n",
    "    @param num: number indicating which angle of the 2D images you want\n",
    "    \"\"\"\n",
    "    #dataset = h5py.File('/hpf/largeprojects/ccm/devin/plastics-data/general/create_2d_array/2D_data_color_4_ds8_uni_split_flips.h5'.format(num), 'r')\n",
    "    dataset = h5py.File('/home/chantal/2D_data_grayscaled_4_ds8_uni.h5', 'r')\n",
    "    data = dataset.get('data_im')\n",
    "    labels = dataset.get('target')\n",
    "\n",
    "    print(data.shape)\n",
    "    #---------------- Pre-Process Data ----------------#\n",
    "    syn_labels = [1,2,3,4,5,6]\n",
    "    plag_labels = [7]\n",
    "    norm_labels = [8]\n",
    "\n",
    "    syn_data = []\n",
    "    plag_data = []\n",
    "    norm_data = []\n",
    "\n",
    "    # separates data into classes\n",
    "    for d,l in zip(data,labels):\n",
    "        if l in syn_labels:\n",
    "            syn_data += [d]\n",
    "        elif l in norm_labels:\n",
    "            norm_data += [d]\n",
    "        elif l in plag_labels:\n",
    "            plag_data += [d]\n",
    "\n",
    "    # change this to train with synostosis/plagiocephaly/normal\n",
    "    X_train = np.asarray(norm_data)\n",
    "    batch = X_train.shape[0]\n",
    "    channels = X_train.shape[3]\n",
    "    height =X_train.shape[1]\n",
    "    width = X_train.shape[2]\n",
    "    X_train = np.reshape(X_train, (batch, channels, height, width))\n",
    "\n",
    "    print(\"X_train shape\", X_train.shape)\n",
    "\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def train_model(data):\n",
    "     wgan = WGAN()\n",
    "     wgan.train(data, epochs=15000, batch_size=128, save=500)\n",
    "     return    \n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    x_train = load_data(4)\n",
    "    train_model(x_train)\n",
    "\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(self, x_train, epoch):\n",
    "   # change this to produce more examples\n",
    "   rows, cols = 5, 5\n",
    "\n",
    "   noise = np.random.normal(0, 1, (rows * cols, self.latent_dim))\n",
    "   # uncomment second gen_imgs to visualize training set imgs\n",
    "   gen_imgs = self.generator.predict(noise)\n",
    "   #gen_imgs = x_train[:rows*cols, :, :, :]\n",
    "\n",
    "   fig, axs = plt.subplots(rows, cols)\n",
    "   c = 0\n",
    "   for i in range(r):\n",
    "       for j in range(c):\n",
    "           axs[i,j].imshow(gen_imgs[c, :,:,0], cmap='gray')\n",
    "           axs[i,j].axis('off')\n",
    "           c += 1\n",
    "   fig.savefig(\"gen/gen_wgan_%d\" % epoch)\n",
    "   plt.close()\n",
    "\n",
    "def sample_losses(self, epoch, gloss, dloss):\n",
    "   plt.figure(figsize=(10, 8\\10))\n",
    "   plt.plot(dloss, label='Discriminator loss')\n",
    "   plt.plot(gloss, label='Generator loss')\n",
    "   plt.xlabel('Epoch')\n",
    "   plt.ylabel('Loss')\n",
    "   plt.legend()\n",
    "   plt.savefig('loss/loss_wgan_%d.png' % epoch)\n",
    "\n",
    "# wasserstein loss function\n",
    "def wasserstein_loss(self, y_true, y_pred):\n",
    "   return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "![](notebook_images/real_2d_2.PNG)\n",
    "## DCGAN\n",
    "![](notebook_images/img_dcgan.png)\n",
    "![](notebook_images/loss_dcgan.png)\n",
    "## WGAN\n",
    "![](notebook_images/gen_wgan_5000.PNG)\n",
    "![](notebook_images/IMG_9881.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D GAN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Add, Reshape\n",
    "from keras.layers import BatchNormalization, Activation, MaxPooling1D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv3D, Deconv3D, Conv1D, UpSampling1D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting loss functions\n",
    "def read_txt(input_path, wgan):\n",
    "    dis_1 = []\n",
    "    dis_2 = []\n",
    "    gen = []\n",
    "    with open(input_path, 'r') as f:\n",
    "        for cnt, line in enumerate(f):\n",
    "            nb = line.split(', ')\n",
    "            dis_1.append(float(nb[1]))\n",
    "            if wgan:\n",
    "                gen.append(float(nb[2]))\n",
    "            else:\n",
    "                dis_2.append(float(nb[2]))\n",
    "                gen.append(float(nb[3]))\n",
    "    if wgan:\n",
    "        return [dis_1], gen\n",
    "    else:\n",
    "        return [dis_1, dis_2], gen\n",
    "\n",
    "\n",
    "def loss_plot(dis, gen, output_path, wgan):\n",
    "    plt.figure()\n",
    "    if wgan:\n",
    "        plt.plot(dis[0], label=\"Critic_loss\")\n",
    "    else:\n",
    "        plt.plot(dis[0], label=\"Discriminator_loss_real\")\n",
    "        plt.plot(dis[1], label=\"Discriminator_loss_fake\")\n",
    "    plt.plot(gen, label=\"Generator_loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc= \"upper right\")\n",
    "\n",
    "    plt.savefig(output_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D DCGAN\n",
    "\n",
    "from dcgan_3d import GAN\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "def train_model(data_path, epochs, batch_size, save, out_file, input_shape, output_shape, train_class):\n",
    "    print(\"Reading the data\")\n",
    "\n",
    "    data = h5py.File(data_path, 'r')\n",
    "    x_data = data.get('data_im')[()]\n",
    "    target = data.get('target')[()]\n",
    "    l = x_data.shape[0]\n",
    "    # Get the class we want to train\n",
    "    index = []\n",
    "    if train_class == 0:\n",
    "        for i in range(l):\n",
    "            if target[i][0] <= 6:\n",
    "                index.append(i)\n",
    "        x_data = x_data[index]\n",
    "    if train_class == 1:\n",
    "        for i in range(l):\n",
    "            if target[i][0] == 7:\n",
    "                index.append(i)\n",
    "        x_data = x_data[index]\n",
    "    if train_class == 2:\n",
    "        for i in range(l):\n",
    "            if target[i][0] == 8:\n",
    "                index.append(i)\n",
    "        x_data = x_data[index]\n",
    "    \n",
    "    \n",
    "    model = GAN(learning_rate_gen=0.00002, learning_rate_disc=0.0002, in_shape=input_shape, out_shape=output_shape)\n",
    "    model.train(x_data, epochs, save, batch_size, out_file)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model for class 0\n",
    "dcgan = train_model(data_path, 2000, 64, 100, 'dcgan_3d.h5', (64, 64, 64, 1), (64, 64, 64, 1), 0)\n",
    "\n",
    "# Plotting the losses:\n",
    "dis_dc, gen_dc = read_txt('results_dcgan.txt')\n",
    "loss_plot(dis_dc, gen_dc, 'losses_dcgan.png', wgan=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/3ddcgan_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sample images : 3 images with 3 different angles (3x3)\n",
    "rows = 3\n",
    "noise = dcgan.generator_noise(rows)\n",
    "generated_images = dcgan.generator.predict(noise)\n",
    "u = generated_images[0].shape[0]\n",
    "\n",
    "gs = gridspec.GridSpec(rows, 3)\n",
    "plt.figure()\n",
    "for i in range(rows):\n",
    "    d = generated_images[i].reshape(u, u, u)\n",
    "    ax = plt.subplot(gs[i, 0])\n",
    "    plt.imshow(np.rot90(d[32,:,:], 2), cmap=plt.cm.get_cmap('gray_r', 20))\n",
    "    plt.clim(0,1)\n",
    "    ax = plt.subplot(gs[i, 1])\n",
    "    plt.imshow(d[:,32,:], cmap=plt.cm.get_cmap('gray_r', 20))\n",
    "    plt.clim(0,1)\n",
    "    ax = plt.subplot(gs[i, 2])\n",
    "    plt.imshow(np.rot90(d[:,:,32]), cmap=plt.cm.get_cmap('gray_r', 20))\n",
    "    plt.colorbar()\n",
    "    plt.clim(0,1)\n",
    "plt.savefig(\"generated_images_dcgan.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/3ddcgan_75.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D WGAN\n",
    "\n",
    "from wgan_3d import WGAN\n",
    "\n",
    "# Train the model for class 0\n",
    "data = h5py.File('/storage/general/data.h5', 'r')\n",
    "x_data = data.get('data_im')[()]\n",
    "target = data.get('target')[()]\n",
    "l = x_data.shape[0]\n",
    "index = []\n",
    "    for i in range(l):\n",
    "    if target[i][0] <= 6:\n",
    "        index.append(i)\n",
    "x_data = x_data[index]\n",
    "\n",
    "wgan = WGAN()\n",
    "wgan.train(x_data, epochs=40000, batch_size=128, sample_interval=100)\n",
    "\n",
    "\n",
    "# Plotting the losses:\n",
    "cri_wg, gen_wg = read_txt('results_wgan.txt')\n",
    "loss_plot(cri_wg, gen_wg, 'losses_wgan.png', wgan=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/3dwgan_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sample images : 3 images with 3 different angles (3x3)\n",
    "rows = 3\n",
    "noise = np.random.normal(0, 1, (3, wgan.latent_dim))\n",
    "generated_images = wgan.generator.predict(noise)\n",
    "u = generated_images[0].shape[0]\n",
    "\n",
    "gs = gridspec.GridSpec(rows, 3)\n",
    "plt.figure()\n",
    "for i in range(rows):\n",
    "    d = generated_images[i].reshape(u, u, u)\n",
    "    ax = plt.subplot(gs[i, 0])\n",
    "    plt.imshow(np.rot90(d[32,:,:], 2), cmap=plt.cm.get_cmap('gray_r', 20))\n",
    "    plt.clim(0,1)\n",
    "    ax = plt.subplot(gs[i, 1])\n",
    "    plt.imshow(d[:,32,:], cmap=plt.cm.get_cmap('gray_r', 20))\n",
    "    plt.clim(0,1)\n",
    "    ax = plt.subplot(gs[i, 2])\n",
    "    plt.imshow(np.rot90(d[:,:,32]), cmap=plt.cm.get_cmap('gray_r', 20))\n",
    "    plt.colorbar()\n",
    "    plt.clim(0,1)\n",
    "plt.savefig(\"generated_images_wgan.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/3dwgan_1500.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D CNN Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l1\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, precision_score, log_loss\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # To disable X rendering so it works on terminal only\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import itertools\n",
    "\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import argparse\n",
    "import time\n",
    "from scipy import interp\n",
    "import warnings\n",
    "# include this to avoid mem leaks issue with tf\n",
    "K.clear_session()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1, 'CPU': 24})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "# Get rid of ALL warning messages\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "#Paths for source data files\n",
    "#Angle 1\n",
    "filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_0_ds8_uni_split.h5'\n",
    "path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_0_color_ds8'\n",
    "\n",
    "#Angle 2\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_1_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_1_color_ds8'\n",
    "\n",
    "#Angle 3\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_2_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_2_color_ds8'\n",
    "\n",
    "#Angle 4\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_3_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_3_color_ds8'\n",
    "\n",
    "#Angle 5\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_4_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_4_color_ds8'\n",
    "\n",
    "#Calculating and Plotting AUROC for each of the predictions along with the micro-average AUROC\n",
    "def test_model(model, testData, testTarget):\n",
    "    y_predict = model.predict(testData)\n",
    "    y_predict_non_category = [np.argmax(t) for t in y_predict]\n",
    "    # print('The confusion matrix of the prediction is, ', confusion_matrix(y_test, y_predict_non_category))\n",
    "    # print('The accuracy of the prediction is, ', accuracy_score(y_test, y_predict_non_category))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    lw = 2\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(testTarget[:, i], y_predict0[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget.ravel(), y_predict0.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(3):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= 3\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='Average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    # plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "    #        label='macro-average ROC curve (area = {0:0.2f})'\n",
    "    #              ''.format(roc_auc[\"macro\"]),\n",
    "    #        color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Model ' + str(m_i) + ': 2D ROC Baseline')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('2D_CNN_NoTL_AUC_' + str(time.time()) + '.png')\n",
    "\n",
    "\n",
    "#Plotting the corresponding confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=20)\n",
    "    plt.colorbar(aspect=3)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize=16,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size=16)\n",
    "    plt.xlabel('Predicted label', size=16)\n",
    "    plt.savefig(path+'/cf_baseline.png')\n",
    "\n",
    "#Grouping outcome labels into 1 of 3 classes (0=Synostosis, 1=Plagiocephaly, 2=Normal)\n",
    "def convert_to_categorical(target):\n",
    "    target[target <= 6] = 0\n",
    "    target[target == 7] = 1\n",
    "    target[target == 8] = 2\n",
    "    target = to_categorical(target)\n",
    "    return target\n",
    "\n",
    "#Loading the data\n",
    "def load_data(filename):\n",
    "    dataset = h5py.File(filename, 'r')\n",
    "    trainData, trainTarget = dataset.get('train_data_im')[()], dataset.get('train_target')[()]\n",
    "    validData, validTarget = dataset.get('valid_data_im')[()], dataset.get('valid_target')[()]\n",
    "    testData, testTarget = dataset.get('test_data_im')[()], dataset.get('test_target')[()]\n",
    "    trainTarget = convert_to_categorical(trainTarget)\n",
    "    validTarget = convert_to_categorical(validTarget)\n",
    "    testTarget = convert_to_categorical(testTarget)\n",
    "    return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "trainData, trainTarget, validData, validTarget, testData, testTarget = load_data(filename)\n",
    "\n",
    "#Creating baseline neural network model to be used without transfer learning\n",
    "def create_model(input_shape, output_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "trainTarget_i = [np.argmax(t) for t in trainTarget]\n",
    "validTarget_i = [np.argmax(t) for t in validTarget]\n",
    "testTarget_i = [np.argmax(t) for t in testTarget]\n",
    "\n",
    "print(\"--- Data structures: --- \")\n",
    "print(\"  Train Data:   \" + str(trainData.shape))\n",
    "print(\"  Train Target: \" + str(trainTarget.shape))\n",
    "print(\"  Valid Data:   \" + str(validData.shape))\n",
    "print(\"  Valid Target: \" + str(validTarget.shape))\n",
    "print(\"  Test Data:    \" + str(testData.shape))\n",
    "print(\"  Test Target:  \" + str(testTarget.shape))\n",
    "print(\"\")\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width = trainData.shape[2]\n",
    "img_height = trainData.shape[1]\n",
    "\n",
    "nb_train_samples = trainData.shape[0]\n",
    "nb_validation_samples = validData.shape[0]\n",
    "#epochs = 1  # need to change these values\n",
    "#batch_size = 1  # need to change these values\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "output_shape = len(trainTarget[0])\n",
    "\n",
    "print(\"--- Size of input & output: --- \")\n",
    "print(\"  Input Shape:  \" + str(input_shape))\n",
    "print(\"  Output Shape: \" + str(output_shape))\n",
    "print(\"\")\n",
    "\n",
    "model = create_model(input_shape, output_shape)\n",
    "\n",
    "#opt = keras.optimizers.SGD(lr=0.00001)\n",
    "opt = keras.optimizers.Adam(lr=0.00004)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(trainTarget_i), trainTarget_i)\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "\n",
    "history=model.fit(\n",
    "    x=trainData,\n",
    "    y=trainTarget,\n",
    "    batch_size=50,\n",
    "    epochs=500,\n",
    "    validation_data=(validData, validTarget),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping_monitor],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#Save the model\n",
    "model.save_weights(path+'/2D_CNN_colour_ds8_baseline.h5')\n",
    "print (model.summary())\n",
    "\n",
    "#Evaluate the Model\n",
    "results = model.evaluate(testData, testTarget)\n",
    "predictions = model.predict_classes(testData)\n",
    "print(model.metrics_names)\n",
    "print(results)\n",
    "\n",
    "# Create Accuracy and Loss Over Time\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "# dict_keys(['loss', 'acc', 'val_acc', 'val_loss'])\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(path+'/NNloss_baseline.png')\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(path+'/NNAccuracy_baseline.png')\n",
    "\n",
    "#calculations for confusion matrix\n",
    "y_hat_train = model.predict_classes(trainData, verbose=1)\n",
    "confusion_matrix_train = confusion_matrix(trainTarget_i, y_hat_train)\n",
    "print(\"Data Distribution for Training Set: \")\n",
    "tr_unique, tr_counts = np.unique(trainTarget_i, return_counts=True)\n",
    "print(dict(zip(tr_unique, tr_counts)))\n",
    "print(\"Confusion Matrix for Training Set: \")\n",
    "print(confusion_matrix_train)\n",
    "print(\"\")\n",
    "\n",
    "y_hat_valid = model.predict_classes(validData, verbose=1)\n",
    "confusion_matrix_valid = confusion_matrix(validTarget_i, y_hat_valid)\n",
    "print(\"Data Distribution for Validation Set: \")\n",
    "va_unique, va_counts = np.unique(validTarget_i, return_counts=True)\n",
    "print(dict(zip(va_unique, va_counts)))\n",
    "print(\"Confusion Matrix for Validation Set: \")\n",
    "print(confusion_matrix_valid)\n",
    "print(\"\")\n",
    "\n",
    "y_hat_test = model.predict_classes(testData, verbose=1)\n",
    "cm = confusion_matrix(testTarget_i, y_hat_test)\n",
    "print(\"Data Distribution for Test Set: \")\n",
    "te_unique, te_counts = np.unique(testTarget_i, return_counts=True)\n",
    "print(dict(zip(te_unique, te_counts)))\n",
    "print(\"Confusion Matrix for Test Set: \")\n",
    "print(cm)\n",
    "print(\"\")\n",
    "plot_confusion_matrix(cm, classes=['Synostosis', 'Plagiocephaly', 'Normal'], title='Confusion Matrix')\n",
    "\n",
    "test_model(model, testData, testTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2D_CNN_Final/2D_CNN_Baseline/Baseline_2D_CNN_1.png)\n",
    "![](2D_CNN_Final/2D_CNN_Baseline/Baseline_2D_CNN_2.png)\n",
    "![](2D_CNN_Final/2D_CNN_Baseline/Baseline_2D_CNN_3.png)\n",
    "![](2D_CNN_Final/2D_CNN_Baseline/Baseline_2D_CNN_4.png)\n",
    "![](2D_CNN_Final/2D_CNN_Baseline/Baseline_2D_CNN_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Ensemble Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "import h5py\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, precision_score, log_loss, roc_auc_score, classification_report, roc_curve, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy import stats\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "# Get rid of ALL warning messages\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.applications import InceptionResNetV2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.layers import Conv3D, MaxPool3D\n",
    "from keras.layers import Input, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# include this to avoid mem leaks issue with tf\n",
    "K.clear_session()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1, 'CPU': 24})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#loading models\n",
    "path='/home/pouriamashouri/project'  # Where the individual graphs will be saved\n",
    "\n",
    "#Paths for source data files\n",
    "#Angle 1\n",
    "filename1 = \"/home/pouriamashouri/project/data/2D_data_color_0_ds8_uni_split_flips.h5\"\n",
    "model_name1 = \"/home/pouriamashouri/project/Models/2D_CNN_colour_0_ds8.h5\"\n",
    "\n",
    "#Angle 2\n",
    "filename2 = \"/home/pouriamashouri/project/data/2D_data_color_1_ds8_uni_split_flips.h5\"\n",
    "model_name2 = \"/home/pouriamashouri/project/Models/2D_CNN_colour_1_ds8.h5\"\n",
    "\n",
    "#Angle 3\n",
    "filename3 = \"/home/pouriamashouri/project/data/2D_data_color_2_ds8_uni_split_flips.h5\"\n",
    "model_name3 = \"/home/pouriamashouri/project/Models/2D_CNN_colour_2_ds8.h5\"\n",
    "\n",
    "#Angle 4\n",
    "filename4 = \"/home/pouriamashouri/project/data/2D_data_color_3_ds8_uni_split_flips.h5\"\n",
    "model_name4 = \"/home/pouriamashouri/project/Models/2D_CNN_colour_3_ds8.h5\"\n",
    "\n",
    "#Angle 5\n",
    "filename5 = \"/home/pouriamashouri/project/data/2D_data_color_4_ds8_uni_split_flips.h5\"\n",
    "model_name5 = \"/home/pouriamashouri/project/Models/2D_CNN_colour_4_ds8.h5\"\n",
    "\n",
    "\n",
    "# Plotting the AUROCs for each of the individual models\n",
    "\n",
    "def test_model(testData, testTarget, m_i, model=None, y_predict=None):\n",
    "    if (y_predict is None):\n",
    "        y_predict = model.predict(testData)\n",
    "        \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    lw = 2\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(testTarget[:, i], y_predict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget.ravel(), y_predict.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(3):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= 3\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='Average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Model ' + str(m_i) + ': 2D ROC Baseline')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig('2D_CNN_Ensemble_AUROC.png')\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Model 0 Confusion matrix', cmap=plt.cm.Greens, m_i=0):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize=20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size=18)\n",
    "    plt.xlabel('Predicted label', size=18)\n",
    "    plt.savefig('2D_CNN_Ensemble_CM.png')\n",
    "\n",
    "\n",
    "#Loading and processing the data for each model\n",
    "def convert_to_categorical(target):\n",
    "    target[target <= 6] = 0\n",
    "    target[target == 7] = 1\n",
    "    target[target == 8] = 2\n",
    "    target = to_categorical(target)\n",
    "    return target\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    dataset = h5py.File(filename, 'r')\n",
    "    trainData, trainTarget = dataset.get('train_data_im')[()], dataset.get('train_target')[()]\n",
    "    validData, validTarget = dataset.get('valid_data_im')[()], dataset.get('valid_target')[()]\n",
    "    testData, testTarget = dataset.get('test_data_im')[()], dataset.get('test_target')[()]\n",
    "    trainTarget = convert_to_categorical(trainTarget)\n",
    "    validTarget = convert_to_categorical(validTarget)\n",
    "    testTarget = convert_to_categorical(testTarget)\n",
    "    return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_prediction(filename, model_name, m_i):\n",
    "    trainData, trainTarget, validData, validTarget, testData, testTarget = load_data(filename)\n",
    "\n",
    "    testTarget_i = [np.argmax(t) for t in testTarget]\n",
    "\n",
    "    # Defining the model input and output shape\n",
    "    img_width = testData.shape[2]\n",
    "    img_height = testData.shape[1]\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    else:\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "    output_shape = len(testTarget[0])\n",
    "\n",
    "    #Defining each of the model CNNs, loading each model, and then compiling the model after the load\n",
    "    conv_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    #Model\n",
    "    model = Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    with CustomObjectScope({'GlorotUniform': glorot_uniform()}):\n",
    "        model.load_weights(model_name)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.00004)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    #Make Predictions\n",
    "    y_hat_test_all = model.predict(testData, verbose=1)\n",
    "    return y_hat_test_all, np.array(testTarget_i)\n",
    "\n",
    "\n",
    "\n",
    "angle1_prediction, angle1_target = make_prediction(filename1, model_name1, 0)\n",
    "angle2_prediction, angle2_target = make_prediction(filename2, model_name2, 1)\n",
    "angle3_prediction, angle3_target = make_prediction(filename3, model_name3, 2)\n",
    "angle4_prediction, angle4_target = make_prediction(filename4, model_name4, 3)\n",
    "angle5_prediction, angle5_target = make_prediction(filename5, model_name5, 4)\n",
    "\n",
    "\n",
    "if ((angle1_target==angle2_target).all() and (angle1_target==angle3_target).all() and (angle1_target==angle4_target).all() and (angle1_target==angle5_target).all()):\n",
    "    print(\"All targets are the same\")\n",
    "else:\n",
    "    print(\"Error, targets dont match. Exiting\")\n",
    "    sys.exit(1)\n",
    "\n",
    "target = angle1_target\n",
    "\n",
    "\n",
    "final_prediction = (angle1_prediction + angle2_prediction + angle3_prediction + angle4_prediction + angle5_prediction)/5\n",
    "final_prediction = final_prediction.reshape(-1,3)\n",
    "final_prediction_c = np.argmax(final_prediction, axis=1).reshape(-1,1)\n",
    "\n",
    "confusion_matrix_ensemble = confusion_matrix(target, final_prediction_c)\n",
    "plot_confusion_matrix(confusion_matrix_ensemble, classes=['Synostosis', 'Plagiocephaly', 'Other'], title='Ensemble Confusion Matrix', m_i=\"ensemble\")\n",
    "target = to_categorical(target)\n",
    "test_model(None, target, m_i=\"ensemble\", model=None, y_predict=final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2D_CNN_Final/2D_CNN_Ensemble/2D_CNN_Ensemble_AUROC.png)\n",
    "![](2D_CNN_Final/2D_CNN_Ensemble/2D_CNN_Ensemble_CM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Transfer Learning CNN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Get rid of ALL warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications import InceptionResNetV2\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout, Input, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.regularizers import l1\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting tools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # To disable X rendering so it works on terminal only\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, recall_score, precision_score, log_loss\n",
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "from scipy import interp\n",
    "\n",
    "# include this to avoid mem leaks issue with tf\n",
    "K.clear_session()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1, 'CPU': 24})\n",
    "sess = tf.Session(config=config)\n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "\n",
    "#Paths for source data files\n",
    "#Angle 1\n",
    "filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_0_ds8_uni_split.h5'\n",
    "path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_0_color_ds8'\n",
    "\n",
    "#Angle 2\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_1_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_1_color_ds8'\n",
    "\n",
    "#Angle 3\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_2_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_2_color_ds8'\n",
    "\n",
    "#Angle 4\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_3_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_3_color_ds8'\n",
    "\n",
    "#Angle 5\n",
    "#filename='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/data/2D_data_color_4_ds8_uni_split.h5'\n",
    "#path='/home/dsingh/Documents/HPF_Files/Plastic_GANs/2D_CNN/Models/2D_4_color_ds8'\n",
    "\n",
    "#Calculating and Plotting AUROC for each of the predictions along with the micro-average AUROC\n",
    "def test_model(model, testData, testTarget):\n",
    "    y_predict = model.predict(testData)\n",
    "    y_predict_non_category = [np.argmax(t) for t in y_predict]\n",
    "    # print('The confusion matrix of the prediction is, ', confusion_matrix(y_test, y_predict_non_category))\n",
    "    # print('The accuracy of the prediction is, ', accuracy_score(y_test, y_predict_non_category))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    lw = 2\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(testTarget[:, i], y_predict[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget.ravel(), y_predict.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(3):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= 3\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(3), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC for 2D Baseline')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig('Baseline_2D_AUC_' + str(time.time()) + '.png')\n",
    "\n",
    "#Plotting the corresponding confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize=20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size=18)\n",
    "    plt.xlabel('Predicted label', size=18)\n",
    "    plt.savefig(path+'/cf.png')\n",
    "\n",
    "#Grouping outcome labels into 1 of 3 classes (0=Synostosis, 1=Plagiocephaly, 2=Normal)\n",
    "def convert_to_categorical(target):\n",
    "    target[target <= 6] = 0\n",
    "    target[target == 7] = 1\n",
    "    target[target == 8] = 2\n",
    "    target = to_categorical(target)\n",
    "    return target\n",
    "\n",
    "#Loading the data\n",
    "def load_data(filename):\n",
    "    dataset = h5py.File(filename, 'r')\n",
    "    trainData, trainTarget = dataset.get('train_data_im')[()], dataset.get('train_target')[()]\n",
    "    validData, validTarget = dataset.get('valid_data_im')[()], dataset.get('valid_target')[()]\n",
    "    testData, testTarget = dataset.get('test_data_im')[()], dataset.get('test_target')[()]\n",
    "    trainTarget = convert_to_categorical(trainTarget)\n",
    "    validTarget = convert_to_categorical(validTarget)\n",
    "    testTarget = convert_to_categorical(testTarget)\n",
    "    return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "trainData, trainTarget, validData, validTarget, testData, testTarget = load_data(filename)\n",
    "\n",
    "#Creating baseline neural network model to be used without transfer learning\n",
    "def create_model(input_shape, output_shape):\n",
    "    conv_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    #conv_base=applications.InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(conv_base)\n",
    "    #model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    #model.add(MaxPooling2D((2, 2)))\n",
    "    #model.add(Dropout(0.8))\n",
    "\n",
    "    #model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.6))\n",
    "\n",
    "    #model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.6))\n",
    "\n",
    "    #model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    #model.add(Dropout(0.8))\n",
    "\n",
    "    #model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "    #model.add(Dropout(0.8))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "trainTarget_i = [np.argmax(t) for t in trainTarget]\n",
    "validTarget_i = [np.argmax(t) for t in validTarget]\n",
    "testTarget_i = [np.argmax(t) for t in testTarget]\n",
    "\n",
    "print(\"--- Data structures: --- \")\n",
    "print(\"  Train Data:   \" + str(trainData.shape))\n",
    "print(\"  Train Target: \" + str(trainTarget.shape))\n",
    "print(\"  Valid Data:   \" + str(validData.shape))\n",
    "print(\"  Valid Target: \" + str(validTarget.shape))\n",
    "print(\"  Test Data:    \" + str(testData.shape))\n",
    "print(\"  Test Target:  \" + str(testTarget.shape))\n",
    "print(\"\")\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width = trainData.shape[2]\n",
    "img_height = trainData.shape[1]\n",
    "\n",
    "nb_train_samples = trainData.shape[0]\n",
    "nb_validation_samples = validData.shape[0]\n",
    "#epochs = 1  # need to change these values\n",
    "#batch_size = 1  # need to change these values\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "output_shape = len(trainTarget[0])\n",
    "\n",
    "print(\"--- Size of input & output: --- \")\n",
    "print(\"  Input Shape:  \" + str(input_shape))\n",
    "print(\"  Output Shape: \" + str(output_shape))\n",
    "print(\"\")\n",
    "\n",
    "model = create_model(input_shape, output_shape)\n",
    "\n",
    "#opt = keras.optimizers.SGD(lr=0.00001)\n",
    "opt = keras.optimizers.Adam(lr=0.00004)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(trainTarget_i), trainTarget_i)\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "\n",
    "history=model.fit(\n",
    "    x=trainData,\n",
    "    y=trainTarget,\n",
    "    batch_size=50,\n",
    "    epochs=500,\n",
    "    validation_data=(validData, validTarget),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping_monitor],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#Save the model\n",
    "model.save_weights(path+'/2D_CNN_colour_ds8.h5')\n",
    "\n",
    "#Evaluate the Model\n",
    "results = model.evaluate(testData, testTarget)\n",
    "predictions = model.predict_classes(testData)\n",
    "print(model.metrics_names)\n",
    "print(results)\n",
    "\n",
    "# Create Accuracy and Loss Over Time\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "# dict_keys(['loss', 'acc', 'val_acc', 'val_loss'])\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(path+'/NNloss.png')\n",
    "\n",
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(path+'/NNAccuracy.png')\n",
    "\n",
    "probs = model.predict_proba(testData) #[:, 1]\n",
    "\n",
    "#calculations for confusion matrix\n",
    "y_hat_train = model.predict_classes(trainData, verbose=1)\n",
    "confusion_matrix_train = confusion_matrix(trainTarget_i, y_hat_train)\n",
    "print(\"Data Distribution for Training Set: \")\n",
    "tr_unique, tr_counts = np.unique(trainTarget_i, return_counts=True)\n",
    "print(dict(zip(tr_unique, tr_counts)))\n",
    "print(\"Confusion Matrix for Training Set: \")\n",
    "print(confusion_matrix_train)\n",
    "print(\"\")\n",
    "\n",
    "y_hat_valid = model.predict_classes(validData, verbose=1)\n",
    "confusion_matrix_valid = confusion_matrix(validTarget_i, y_hat_valid)\n",
    "print(\"Data Distribution for Validation Set: \")\n",
    "va_unique, va_counts = np.unique(validTarget_i, return_counts=True)\n",
    "print(dict(zip(va_unique, va_counts)))\n",
    "print(\"Confusion Matrix for Validation Set: \")\n",
    "print(confusion_matrix_valid)\n",
    "print(\"\")\n",
    "\n",
    "y_hat_test = model.predict_classes(testData, verbose=1)\n",
    "cm = confusion_matrix(testTarget_i, y_hat_test)\n",
    "print(\"Data Distribution for Test Set: \")\n",
    "te_unique, te_counts = np.unique(testTarget_i, return_counts=True)\n",
    "print(dict(zip(te_unique, te_counts)))\n",
    "print(\"Confusion Matrix for Test Set: \")\n",
    "print(cm)\n",
    "print(\"\")\n",
    "plot_confusion_matrix(cm, classes=['Synostosis', 'Plagiocephaly', 'Other'], title='Confusion Matrix')\n",
    "\n",
    "test_model(model, testData, testTarget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_1.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_1_CM.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_2.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_2_CM.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_3.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_3_CM.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_4.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_4_CM.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN__TL_5.png)\n",
    "![](2D_CNN_Final/2D_CNN_TransferLearning/2D_CNN_TL_5_CM.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D ResNet-18 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to one GPU in multi-GPU systems\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from collections import Counter\n",
    "import h5py\n",
    "from itertools import cycle, product\n",
    "from keras import applications\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from resnet3d import Resnet3DBuilder # https://github.com/JihongJu/keras-resnet3d\n",
    "from scipy import interp\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import time\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Greens):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size=24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size=14)\n",
    "    plt.yticks(tick_marks, classes, size=14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    # Labeling the plot\n",
    "    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize=20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size=18)\n",
    "    plt.xlabel('Predicted label', size=18)\n",
    "    plt.savefig(output_dir + '/3D_ResNet_Confusion_Matrix.png')\n",
    "\n",
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.savefig(output_dir + '/3D_ResNet_Acc_Loss.png')\n",
    "\n",
    "def create_train_test(datapath, test_size=0.3):\n",
    "    with h5py.File(datapath, 'r') as dataset:\n",
    "        data = dataset.get('data_im')[()]\n",
    "        target = dataset.get('target')[()]\n",
    "        for i in range(len(target)):\n",
    "            if target[i][0] <= 6:\n",
    "                target[i][0] = 0\n",
    "            if target[i][0] == 7:\n",
    "                target[i][0] = 1\n",
    "            if target[i][0] == 8:\n",
    "                target[i][0] = 2\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
    "                                                            test_size=test_size)\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],\n",
    "                                  X_train.shape[2], X_train.shape[3], 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],\n",
    "                                X_test.shape[2], X_test.shape[3], 1)\n",
    "        y_train = to_categorical(y_train, 3)\n",
    "        y_test = to_categorical(y_test, 3)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Setup the output directory and copy this same script to the directory for future reference\n",
    "output_dir = \"../output/\" + str(time.strftime(\"%m-%d-%H:%M\"))\n",
    "os.mkdir(output_dir)\n",
    "copy(__file__, output_dir)\n",
    "\n",
    "# Download the data from: /hpf/largeprojects/ccm/devin/plastics-data/???\n",
    "datapath = \"../data/data.h5\"\n",
    "test_size = 0.20\n",
    "X_train, X_test, Y_train, Y_test = create_train_test(datapath, test_size)\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "model = Resnet3DBuilder.build_resnet_18((128, 128, 128, 1), 3)\n",
    "adam = Adam(lr=0.000003)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "earlystop = EarlyStopping(monitor = 'val_acc',\n",
    "                          min_delta = 0.01,\n",
    "                          patience = 3,\n",
    "                          verbose = 2,\n",
    "                          restore_best_weights = True)\n",
    "callback_list = [earlystop]\n",
    "\n",
    "model_info = model.fit(X_train, Y_train,\n",
    "                       epochs = 25,\n",
    "                       batch_size = 8,\n",
    "                       validation_split = 0.10,\n",
    "                       callbacks = callback_list)\n",
    "\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))\n",
    "\n",
    "plot_model_history(model_info)\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "Y_hat_test = model.predict(X_test, verbose=1)\n",
    "cm = confusion_matrix(Y_test.argmax(axis=1), Y_hat_test.argmax(axis=1))\n",
    "print(\"Data Distribution for Test Set: \")\n",
    "te_unique, te_counts = np.unique(Y_test, return_counts=True)\n",
    "print(dict(zip(te_unique, te_counts)))\n",
    "print(\"Confusion Matrix for Test Set: \")\n",
    "print(cm)\n",
    "print(\"\")\n",
    "plot_confusion_matrix(cm, classes=['<6', '7', '8'], title='Confusion Matrix', normalize=True)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "lw = 2\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test[:, i], Y_hat_test[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), Y_hat_test.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(3)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(3):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= 3\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(3), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                   ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC for 3D ResNet')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig(output_dir + '/3D_ResNet_AUC.png')\n",
    "\n",
    "\n",
    "model.save(output_dir + '/model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/3D_ResNet_AUROC.png)\n",
    "![](notebook_images/3D_ResNet_CM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D ResNet-50 with Transfer Learning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict to one GPU in multi-GPU systems\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from collections import Counter\n",
    "import h5py\n",
    "from keras import applications\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from resnet3d import Resnet3DBuilder, basic_block # https://github.com/JihongJu/keras-resnet3d\n",
    "from shutil import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from presentation_plots import plot_confusion_matrix, plot_rocauc, plot_model_history\n",
    "\n",
    "\n",
    "def create_train_test(datapath, test_size=0.3):\n",
    "    with h5py.File(datapath, 'r') as dataset:\n",
    "        data = dataset.get('data_im')[()]\n",
    "        target = dataset.get('target')[()]\n",
    "        print(data.shape)\n",
    "        print(target.shape)\n",
    "\n",
    "        for i in range(len(target)):\n",
    "            if target[i][0] <= 6:\n",
    "                target[i][0] = 0\n",
    "            if target[i][0] == 7:\n",
    "                target[i][0] = 1\n",
    "            if target[i][0] == 8:\n",
    "                target[i][0] = 2\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
    "                                                            test_size=test_size)\n",
    "        X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],\n",
    "                                  X_train.shape[2], X_train.shape[3], 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],\n",
    "                                X_test.shape[2], X_test.shape[3], 1)\n",
    "        y_train = to_categorical(y_train, 3)\n",
    "        y_test = to_categorical(y_test, 3)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Setup the output directory and copy this same script to the directory for future reference\n",
    "output_dir = \"output/\" + str(time.strftime(\"%m-%d-%H:%M\"))\n",
    "os.mkdir(output_dir)\n",
    "copy(__file__, output_dir)\n",
    "\n",
    "# Download the data from: /hpf/largeprojects/ccm/devin/plastics-data/???\n",
    "datapath = \"data/data.h5\"\n",
    "test_size = 0.20\n",
    "X_train, X_test, Y_train, Y_test = create_train_test(datapath, test_size)\n",
    "\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "\n",
    "model = Resnet3DBuilder.build_resnet_50((128, 128, 128, 1), 3)\n",
    "# model = Resnet3DBuilder.build((128, 128, 128, 1), 3, basic_block, [1, 1, 1, 1], reg_factor=1e-4)\n",
    "\n",
    "# Uncomment the following block to do transfer learning!\n",
    "model.layers[-1].name = \"dense_resnet_1\" # Rename final dense layer so correct number of output classes is used (3 instead of MNIST 10)\n",
    "# MNIST build_resnet_18\n",
    "# model.load_weights('/home/carsonmclean/dev/csc2541/csc2541/output/04-03-03:37/model.h5',\n",
    "#                    by_name = True)\n",
    "# MNIST [1,1,1,1]\n",
    "# model.load_weights('/home/carsonmclean/dev/csc2541/csc2541/output/04-09-00:25/model.h5',\n",
    "#                    by_name = True)\n",
    "\n",
    "# ModelNet40/PointNet build_resnet_18 dim = 64\n",
    "# model.load_weights('/home/carsonmclean/dev/csc2541/csc2541/output/04-09-17:40/model.h5',\n",
    "#                    by_name = True)\n",
    "# ModelNet40/PointNet build_resnet_18 dim = 128\n",
    "# model.load_weights('/home/carsonmclean/dev/csc2541/csc2541/output/04-10-08:41/model.h5',\n",
    "#                    by_name = True)\n",
    "# ModelNet40/PointNet build_resnet_50 dim = 64\n",
    "model.load_weights('/home/carsonmclean/dev/csc2541/csc2541/output/04-10-08:54/model.h5',\n",
    "                   by_name = True)\n",
    "\n",
    "# End of transfer learning\n",
    "\n",
    "adam = Adam(lr=0.000001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "earlystop = EarlyStopping(monitor = 'val_acc',\n",
    "                          min_delta = 0.01,\n",
    "                          patience = 6,\n",
    "                          verbose = 2,\n",
    "                          restore_best_weights = True)\n",
    "callback_list = [earlystop]\n",
    "\n",
    "model_info = model.fit(X_train, Y_train,\n",
    "                       epochs = 50,\n",
    "                       batch_size = 4,\n",
    "                       validation_split = 0.10,\n",
    "                       callbacks = callback_list)\n",
    "\n",
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))\n",
    "\n",
    "Y_hat_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "plot_confusion_matrix(Y_test,\n",
    "                      Y_hat_test,\n",
    "                      ['Synostosis', 'Plagiocephaly', 'Normal'],\n",
    "                      \"3D ResNet\",\n",
    "                      output_dir)\n",
    "plot_model_history(model_info, \"3D ResNet\", output_dir)\n",
    "plot_rocauc(Y_test, Y_hat_test, \"3D ResNet\", output_dir)\n",
    "\n",
    "\n",
    "model.save(output_dir + '/model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](notebook_images/3D_ResNet-50_TL_AUC.png)\n",
    "![](notebook_images/3D_ResNet-50_TL_Confusion_Matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
